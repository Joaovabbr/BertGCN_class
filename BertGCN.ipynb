{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafff4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset # <-- Usaremos o DataLoader padrão\n",
    "\n",
    "# --- Hugging Face Transformers (BERT) ---\n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "# Desliga os warnings do Hugging Face (opcional, mas limpa a saída)\n",
    "logging.set_verbosity_error() \n",
    "\n",
    "# --- PyTorch Geometric (GNNs) ---\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# --- NLTK (Stopwords) ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Scikit-learn (Métricas) ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Verificação de Hardware (CRÍTICO) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baixar Stopwords do NLTK ---\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "print(f\"Carregadas {len(stop_words_set)} stopwords do NLTK.\")\n",
    "\n",
    "# --- Constantes do Projeto ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "DATA_DIR = \"./data\" # !!! MUDE ESTE CAMINHO !!!\n",
    "\n",
    "# --- Hiperparâmetros de Treinamento ---\n",
    "# 2e-5 é o learning rate padrão para fine-tuning de BERT\n",
    "LEARNING_RATE = 2e-5 \n",
    "BATCH_SIZE = 2 # Ajuste se tiver erros de memória (pode tentar 8 ou 12)\n",
    "NUM_EPOCHS = 4  # Fine-tuning converge rápido. Comece com 3-5\n",
    "HIDDEN_DIM = 256 # Dimensão das camadas da GNN\n",
    "MAX_LENGTH = 200 # Limite máximo de tokens do BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeaa9a",
   "metadata": {},
   "source": [
    "### Para Carregar Dataset IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# --- 1. Carregar Dataset IMDB ---\n",
    "print(\"Baixando e carregando dataset IMDB...\")\n",
    "# Isso baixa o dataset (pode demorar um pouco na primeira vez)\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"Formatando dados...\")\n",
    "# O IMDB já vem com 'text' e 'label' (0=neg, 1=pos)\n",
    "# E já vem pré-dividido em 'train' e 'test'\n",
    "train_texts_cleaned = imdb_dataset['train']['text']\n",
    "train_labels = imdb_dataset['train']['label']\n",
    "\n",
    "test_texts_cleaned = imdb_dataset['test']['text']\n",
    "test_labels = imdb_dataset['test']['label']\n",
    "\n",
    "# --- 2. Definir Classes (Hardcoded) ---\n",
    "num_classes = 8 # IMDB é binário (negativo/positivo)\n",
    "\n",
    "print(f\"--- Dados do IMDB Carregados ---\")\n",
    "print(f\"Total de classes: {num_classes}\")\n",
    "print(f\"Documentos de treino: {len(train_texts_cleaned)}\")\n",
    "print(f\"Documentos de teste: {len(test_texts_cleaned)}\")\n",
    "\n",
    "# --- 3. Criar a classe TextGraphDataset ---\n",
    "# (Esta classe é da sua Célula 4 original, apenas a copiamos para cá)\n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# --- 4. Criar instâncias do Dataset ---\n",
    "train_dataset = TextGraphDataset(train_texts_cleaned, train_labels)\n",
    "test_dataset = TextGraphDataset(test_texts_cleaned, test_labels)\n",
    "\n",
    "# --- 5. Carregar Tokenizador (da Célula 4 original) ---\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def build_token_graph(num_tokens, window_size=2):\n",
    "    \"\"\"Cria um grafo de janela deslizante para 'num_tokens'.\"\"\"\n",
    "    edge_index_list = []\n",
    "    for i in range(num_tokens):\n",
    "        for j in range(max(0, i - window_size), min(num_tokens, i + window_size)):\n",
    "            if i != j:\n",
    "                edge_index_list.append([i, j])\n",
    "                \n",
    "    if not edge_index_list:\n",
    "        return torch.tensor([[0], [0]], dtype=torch.long)\n",
    "    \n",
    "    return torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a2052",
   "metadata": {},
   "source": [
    "### Para Utilizar Dataset R8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daa02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filepath):\n",
    "    \"\"\"Lê os arquivos de dados do R8 (formato: label\\ttexto).\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                label, text = parts\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpa o texto (remove pontuação, números, stopwords).\"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text) # Mantém apenas letras\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if w not in stop_words_set and len(w) > 2]\n",
    "    return \" \".join(cleaned_words) # Retorna o texto limpo como string\n",
    "\n",
    "# --- Carregar Dados ---\n",
    "print(\"Carregando e limpando dados...\")\n",
    "train_texts_raw, train_labels_str = load_data_and_labels(os.path.join(DATA_DIR, 'train.txt'))\n",
    "test_texts_raw, test_labels_str = load_data_and_labels(os.path.join(DATA_DIR, 'test.txt'))\n",
    "\n",
    "train_texts_cleaned = [clean_text(text) for text in train_texts_raw]\n",
    "test_texts_cleaned = [clean_text(text) for text in test_texts_raw]\n",
    "\n",
    "# --- Mapeamento de Labels ---\n",
    "labels_unique = sorted(list(set(train_labels_str)))\n",
    "num_classes = len(labels_unique)\n",
    "label_map = {label: i for i, label in enumerate(labels_unique)}\n",
    "\n",
    "train_labels = [label_map[label] for label in train_labels_str]\n",
    "test_labels = [label_map[label] for label in test_labels_str]\n",
    "\n",
    "print(f\"Total de classes: {num_classes} ({labels_unique[0]}, {labels_unique[1]}, ...)\")\n",
    "print(f\"Documentos de treino: {len(train_texts_cleaned)}\")\n",
    "print(f\"Documentos de teste: {len(test_texts_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65daa0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o tokenizador globalmente\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def build_token_graph(num_tokens, window_size=2):\n",
    "    \"\"\"Cria um grafo de janela deslizante para 'num_tokens'.\"\"\"\n",
    "    edge_index_list = []\n",
    "    for i in range(num_tokens):\n",
    "        for j in range(max(0, i - window_size), min(num_tokens, i + window_size)):\n",
    "            if i != j:\n",
    "                edge_index_list.append([i, j])\n",
    "                \n",
    "    if not edge_index_list:\n",
    "        # Garante que o grafo não esteja vazio (para [CLS], [SEP])\n",
    "        return torch.tensor([[0], [0]], dtype=torch.long)\n",
    "    \n",
    "    return torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "\n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retorna o texto bruto e a label\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Cria as instâncias do Dataset\n",
    "train_dataset = TextGraphDataset(train_texts_cleaned, train_labels)\n",
    "test_dataset = TextGraphDataset(test_texts_cleaned, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_bert_gnn(batch):\n",
    "    \"\"\"\n",
    "    Função customizada para empacotar o lote.\n",
    "    'batch' é uma lista de tuplas (texto, label)\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # 1. Processa para o BERT\n",
    "    # Tokeniza o lote de textos com padding\n",
    "    bert_inputs = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 2. Processa para a GNN\n",
    "    data_list = []\n",
    "    for i in range(len(texts)):\n",
    "        # Pega o número real de tokens (sem padding)\n",
    "        # O 'attention_mask' é 1 para tokens reais, 0 para padding\n",
    "        num_tokens = int(bert_inputs['attention_mask'][i].sum())\n",
    "        \n",
    "        # Constrói o grafo de tokens para este item\n",
    "        edge_index = build_token_graph(num_tokens)\n",
    "        \n",
    "        # O 'y' (label) para este item\n",
    "        y = torch.tensor(labels[i], dtype=torch.long)\n",
    "        \n",
    "        # Criamos um Data object sem 'x'. O 'x' virá do BERT\n",
    "        data_list.append(Data(edge_index=edge_index, y=y))\n",
    "    \n",
    "    # 3. Empacota a GNN\n",
    "    # 'Batch.from_data_list' cria o batch de grafos para o PyG\n",
    "    pyg_batch = Batch.from_data_list(data_list)\n",
    "    \n",
    "    # 4. Retorna tudo\n",
    "    return {\n",
    "        'bert_inputs': bert_inputs, # Contém input_ids, attention_mask\n",
    "        'pyg_batch': pyg_batch        # Contém edge_index, y, batch_map\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45785bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa o DataLoader padrão do PyTorch, mas com nosso collate_fn\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_bert_gnn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_bert_gnn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"--- DataLoaders Prontos ---\")\n",
    "print(f\"Total de lotes de treino: {len(train_loader)}\")\n",
    "print(f\"Total de lotes de teste: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN_FineTune(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(HybridGNN_FineTune, self).__init__()\n",
    "        \n",
    "        # 1. Camada BERT (Será treinada)\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        bert_output_dim = self.bert.config.hidden_size # 768\n",
    "        \n",
    "        # 2. Camadas GNN\n",
    "        self.gcn1 = GCNConv(bert_output_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # 3. Pooling (Agrega os nós de tokens em um vetor de documento)\n",
    "        self.pool = global_mean_pool\n",
    "        \n",
    "        # 4. Camada de Classificação Final\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, bert_inputs, pyg_batch):\n",
    "        # Move os inputs do BERT para a GPU\n",
    "        input_ids = bert_inputs['input_ids'].to(device)\n",
    "        attention_mask = bert_inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Move os inputs da GNN para a GPU\n",
    "        edge_index = pyg_batch.edge_index.to(device)\n",
    "        batch_map = pyg_batch.batch.to(device) # Mapa de nós para grafos\n",
    "        \n",
    "        # 1. Passa pelo BERT (Fine-tuning)\n",
    "        # Pega os embeddings de CADA token\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state # Shape: [batch_size, seq_len, 768]\n",
    "\n",
    "        # 2. Prepara Features para a GNN\n",
    "        # A GNN espera um 'x' (features) no formato [Total de Nós, Dim Features]\n",
    "        # Precisamos \"desenrolar\" os embeddings do BERT\n",
    "        \n",
    "        # Pega apenas os embeddings dos tokens reais (ignora padding)\n",
    "        # O 'batch_map' do PyG nos diz quais tokens pertencem a qual doc\n",
    "        # Esta é uma forma eficiente de fazer isso:\n",
    "        active_token_embeddings = token_embeddings[attention_mask > 0]\n",
    "        \n",
    "        # 'active_token_embeddings' é agora o nosso 'x' para a GNN\n",
    "        \n",
    "        # 3. Passa pela GNN\n",
    "        h = self.gcn1(active_token_embeddings, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # 4. Pooling (Agrega os nós em um vetor por grafo)\n",
    "        # h: (Total de Nós no Batch, hidden_dim)\n",
    "        # graph_embedding: (Tamanho do Batch, hidden_dim)\n",
    "        graph_embedding = self.pool(h, batch_map)\n",
    "        \n",
    "        # 5. Classificação\n",
    "        graph_embedding = self.dropout(graph_embedding)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- Instanciando o Modelo ---\n",
    "model = HybridGNN_FineTune(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes, # 8\n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"--- Arquitetura do Modelo (com Fine-Tuning) ---\")\n",
    "# print(model) # Descomente se quiser ver a arquitetura completa\n",
    "print(f\"Modelo {type(model).__name__} instanciado e movido para {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd284cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Iniciando Treinamento por {NUM_EPOCHS} épocas ---\")\n",
    "print(f\"Otimizador: Adam, LR: {LEARNING_RATE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train() # Coloca o modelo em modo de treino\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # O batch é um dicionário {'bert_inputs': ..., 'pyg_batch': ...}\n",
    "        bert_inputs = batch['bert_inputs']\n",
    "        pyg_batch = batch['pyg_batch']\n",
    "        \n",
    "        # Pega as labels corretas\n",
    "        labels = pyg_batch.y.to(device)\n",
    "        \n",
    "        # Zera os gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(bert_inputs, pyg_batch)\n",
    "        \n",
    "        # Calcula o loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass (Calcula os gradientes para GNN e BERT)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Otimiza (Ajusta os pesos)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Época {epoch+1}, Lote {i+1}/{len(train_loader)} | Loss Lote: {loss.item():.4f}\")\n",
    "\n",
    "    # --- Fim da Época ---\n",
    "    end_time = time.time()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nÉPOCA {epoch+1:02d}/{NUM_EPOCHS} Completa\")\n",
    "    print(f\"  Tempo: {epoch_time:.2f}s\")\n",
    "    print(f\"  Loss Média (Treino): {avg_loss:.4f}\")\n",
    "\n",
    "print(\"--- Treinamento Concluído ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Iniciando Avaliação no Conjunto de Teste (Indutivo) ---\")\n",
    "\n",
    "model.eval() # Coloca o modelo em modo de avaliação\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Loop de avaliação com limpeza de cache\n",
    "with torch.no_grad(): # Desliga o cálculo de gradientes\n",
    "    \n",
    "    # Adiciona um timer para ver a velocidade\n",
    "    start_time = time.time() \n",
    "    \n",
    "    for i, batch in enumerate(test_loader):\n",
    "        bert_inputs = batch['bert_inputs']\n",
    "        pyg_batch = batch['pyg_batch']\n",
    "        \n",
    "        labels = pyg_batch.y.to(device)\n",
    "        \n",
    "        # Faz a previsão\n",
    "        logits = model(bert_inputs, pyg_batch)\n",
    "        \n",
    "        # Pega a classe com maior pontuação\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        \n",
    "        # Salva para cálculo final\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- [NOVO] LIMPEZA MANUAL DE MEMÓRIA ---\n",
    "        # Deleta as referências aos tensores da GPU\n",
    "        del bert_inputs, pyg_batch, labels, logits, predicted\n",
    "        \n",
    "        # Força o PyTorch a esvaziar o cache da VRAM\n",
    "        torch.cuda.empty_cache()\n",
    "        # ---------------------------------------\n",
    "\n",
    "# --- Calcula a Acurácia Final ---\n",
    "end_time = time.time()\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "\n",
    "print(f\"\\nTempo total de avaliação: {(end_time - start_time):.2f} segundos\")\n",
    "print(\"\\n--- Resultados Finais (com Fine-Tuning) ---\")\n",
    "print(f\"Total de Amostras de Teste: {len(all_true_labels)}\")\n",
    "print(f\"Acurácia no Teste: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definir o caminho para salvar ---\n",
    "dataset = \"imdb\"  # Altere conforme o dataset utilizado\n",
    "MODEL_SAVE_PATH = f\"{dataset}_hybrid_gnn_finetuned.pth\"\n",
    "\n",
    "# 2. Salvar o 'state_dict' (os pesos)\n",
    "# Isso salva apenas os parâmetros aprendidos, não a arquitetura\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"\\n--- Modelo Salvo ---\")\n",
    "print(f\"Os parâmetros do modelo foram salvos em: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eacc7e",
   "metadata": {},
   "source": [
    "### Apenas para carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definir o caminho do modelo salvo ---\n",
    "MODEL_SAVE_PATH = \"r8_hybrid_gnn_finetuned.pth\"\n",
    "num_classes = 8\n",
    "\n",
    "# --- 2. Recriar a arquitetura do modelo ---\n",
    "# Você DEVE instanciar a classe do modelo primeiro.\n",
    "# (Certifique-se de que HIDDEN_DIM e num_classes estão definidos pela Célula 2)\n",
    "print(\"Instanciando a arquitetura do modelo...\")\n",
    "loaded_model = HybridGNN_FineTune(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# --- 3. Carregar os pesos salvos ---\n",
    "print(f\"Carregando pesos de {MODEL_SAVE_PATH}...\")\n",
    "loaded_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "\n",
    "# --- 4. Colocar em modo de avaliação (MUITO IMPORTANTE!) ---\n",
    "# Isso desliga o dropout e outras camadas específicas de treino.\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"\\n--- Modelo Carregado e Pronto! ---\")\n",
    "print(\"O 'loaded_model' está pronto para ser usado para avaliação (Célula 9).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
