{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafff4d2",
   "metadata": {},
   "source": [
    "# Projeto Final - ***Natural Language Processing***\n",
    "\n",
    "\n",
    "João Victor de Almeida Braga\n",
    "\n",
    "Sidney Barbosa de Oliveira\n",
    "\n",
    "Diogo Gindler Diniz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89f096",
   "metadata": {},
   "source": [
    "**Introdução**\n",
    "\n",
    "Muitos modelos de alto desempenho exigem que todos os dados, incluindo os de teste, sejam \"vistos\" durante o treinamento (aprendizado transdutivo). Essa abordagem, embora eficaz em benchmarks, é inviável para aplicações do mundo real que exigem a classificação de dados novos e nunca vistos. A solução prática e necessária para a generalização é o aprendizado indutivo.\n",
    "\n",
    "**Objetivo**\n",
    "\n",
    "O objetivo central do projeto é investigar a viabilidade de um modelo híbrido BERT-GNN indutivo. Os treinos do modelo são feitos apenas com dados conhecidos, aprendendo a generalizar de forma robusta. Assim, pretendemos verificar se o modelo consegue manter o alto desempenho dos baselines transdutivos, ao mesmo tempo que adquire flexibilidade e aplicabilidade prática de um modelo indutivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports do pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "\n",
    "# Desliga os warnings do Hugging Face \n",
    "logging.set_verbosity_error() \n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# NLTK \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Scikit-learn (Métricas) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Verificação de Hardware (CRÍTICO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords do NLTK \n",
    "nltk.download('stopwords')\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "print(f\"Carregadas {len(stop_words_set)} stopwords do NLTK.\")\n",
    "\n",
    "# Constantes do Projeto\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "DATA_DIR = \"./data\" \n",
    "\n",
    "# Hiperparâmetros de Treinamento \n",
    "LEARNING_RATE = 2e-5 \n",
    "BATCH_SIZE = 2 \n",
    "NUM_EPOCHS = 4  \n",
    "HIDDEN_DIM = 256 \n",
    "MAX_LENGTH = 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeaa9a",
   "metadata": {},
   "source": [
    "### Preparação Inicial do DataSet e Definição de Estruturas\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Preparamos o dataset IMDB para um modelo de Rede Neural Gráfica (GNN). Primeiramente, ele carrega e inspeciona os documentos de treino e teste, definindo estruturas de dados com a classe TextGraphDataset para encapsular textos e rótulos. Em seguida, inicializa o BertTokenizer para processamento textual. \n",
    "\n",
    "O build_token_graph implementa a lógica de conversão de sequências de tokens em grafos, ou seja, ele estabelece arestas entre tokens adjacentes dentro de uma janela de co-ocorrência, fundamental para modelar a relação de proximidade entre palavras e permitir a propagação de mensagens do GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregando DataSet do IMDB\n",
    "print(\"Baixando e carregando dataset IMDB...\")\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "train_texts_cleaned = imdb_dataset['train']['text']\n",
    "train_labels = imdb_dataset['train']['label']\n",
    "\n",
    "test_texts_cleaned = imdb_dataset['test']['text']\n",
    "test_labels = imdb_dataset['test']['label']\n",
    "\n",
    "# Definindo classes\n",
    "num_classes = 8 \n",
    "\n",
    "print(f\"Total de classes: {num_classes}\")\n",
    "print(f\"Documentos de treino: {len(train_texts_cleaned)}\")\n",
    "print(f\"Documentos de teste: {len(test_texts_cleaned)}\")\n",
    "\n",
    "# Classe TextGraphDataset \n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Instâncias do Dataset \n",
    "train_dataset = TextGraphDataset(train_texts_cleaned, train_labels)\n",
    "test_dataset = TextGraphDataset(test_texts_cleaned, test_labels)\n",
    "\n",
    "# Tokenizador \n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "def build_token_graph(num_tokens, window_size=2):\n",
    "    edge_index_list = []\n",
    "    for i in range(num_tokens):\n",
    "        for j in range(max(0, i - window_size), min(num_tokens, i + window_size)):\n",
    "            if i != j:\n",
    "                edge_index_list.append([i, j])\n",
    "                \n",
    "    if not edge_index_list:\n",
    "        return torch.tensor([[0], [0]], dtype=torch.long)\n",
    "    \n",
    "    return torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a2052",
   "metadata": {},
   "source": [
    "### Preparação e Limpeza do Dataset R8\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Este código implementa o pré-processamento do dataset R8: ele carrega os dados, executa uma limpeza no texto: removendo pontuações, números, stopwords e palavras curtas. Em seguida, mapeia os rótulos textuais únicos para índices numéricos sequenciais, preparando os conjuntos de treino e teste para um modelo de classificação com $N$ classes (onde $N$ é o total de rótulos únicos encontrados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daa02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filepath):\n",
    "    \"\"\"Lê os arquivos de dados do R8\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                label, text = parts\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpa o texto\"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if w not in stop_words_set and len(w) > 2]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "# Carregamento de Dados\n",
    "train_texts_raw, train_labels_str = load_data_and_labels(os.path.join(DATA_DIR, 'train.txt'))\n",
    "test_texts_raw, test_labels_str = load_data_and_labels(os.path.join(DATA_DIR, 'test.txt'))\n",
    "\n",
    "train_texts_cleaned = [clean_text(text) for text in train_texts_raw]\n",
    "test_texts_cleaned = [clean_text(text) for text in test_texts_raw]\n",
    "\n",
    "# Mapeamento de Labels\n",
    "labels_unique = sorted(list(set(train_labels_str)))\n",
    "num_classes = len(labels_unique)\n",
    "label_map = {label: i for i, label in enumerate(labels_unique)}\n",
    "\n",
    "train_labels = [label_map[label] for label in train_labels_str]\n",
    "test_labels = [label_map[label] for label in test_labels_str]\n",
    "\n",
    "print(f\"Total de classes: {num_classes} ({labels_unique[0]}, {labels_unique[1]}, ...)\")\n",
    "print(f\"Documentos de treino: {len(train_texts_cleaned)}\")\n",
    "print(f\"Documentos de teste: {len(test_texts_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2167f",
   "metadata": {},
   "source": [
    "### Configuração De Batch Híbrido\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Esta função, collate_fn_bert_gnn, é uma rotina de agrupamento customizada crucial para modelos híbridos que combinam BERT e Redes Neurais Gráficas (GNN). Ela recebe um batch de textos e rótulos e realiza o processamento duplo: primeiro, tokeniza e padroniza os textos para criar os inputs do BERT (bert_inputs), e em seguida, para cada item, usa a máscara de atenção do BERT para determinar o número real de tokens e constrói o edge_index (a estrutura de adjacência de grafo) usando a função build_token_graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_bert_gnn(batch):\n",
    "\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Tokeniza o lote de textos com padding\n",
    "    bert_inputs = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Processa para a GNN\n",
    "    data_list = []\n",
    "    for i in range(len(texts)):\n",
    "        # Pega o número real de tokens \n",
    "        num_tokens = int(bert_inputs['attention_mask'][i].sum())\n",
    "        \n",
    "        # Constrói o grafo de tokens para este item\n",
    "        edge_index = build_token_graph(num_tokens)\n",
    "        \n",
    "        # O y (label) para este item\n",
    "        y = torch.tensor(labels[i], dtype=torch.long)\n",
    "        \n",
    "        # Criamos um Data object sem x. O x virá do BERT\n",
    "        data_list.append(Data(edge_index=edge_index, y=y))\n",
    "    \n",
    "    # Empacota a GNN\n",
    "    pyg_batch = Batch.from_data_list(data_list)\n",
    "    \n",
    "    # Retorna tudo\n",
    "    return {\n",
    "        'bert_inputs': bert_inputs, \n",
    "        'pyg_batch': pyg_batch       \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd115b5d",
   "metadata": {},
   "source": [
    "### Inicialização dos DataLoaders\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Criamos os DataLoaders de treino e teste do PyTorch. A configuração chave é usar a função collate_fn_bert_gnn para garantir que os dados sejam agrupados corretamente para o modelo híbrido BERT-GNN. O loader de treino embaralha os dados, e o de teste não. O código finaliza exibindo o número de lotes (batches) disponíveis em cada conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45785bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa o DataLoader padrão do PyTorch, mas com nosso collate_fn\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_bert_gnn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_bert_gnn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Total de lotes de treino: {len(train_loader)}\")\n",
    "print(f\"Total de lotes de teste: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b9c7e",
   "metadata": {},
   "source": [
    "### Modelo Híbrido BERT-GNN para Fine-Tuning \n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Definimos e inicializamos a arquitetura HybridGNN_FineTune, um modelo que integra o BERT para extrair embeddings contextuais de cada token (no modo fine-tuning) e duas camadas GCN para modelar as relações entre tokens. No método forward, o modelo filtra o padding do output do BERT para obter features de nó, processa-as pela GNN, utiliza o pooling para condensar a informação do grafo em um vetor de documento, e finaliza com uma camada linear para a classificação.\n",
    "\n",
    "Por fim, o código instancia o modelo e define a função de perda e o otimizador, preparando a estrutura para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGNN_FineTune(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_prob=0.5):\n",
    "        super(HybridGNN_FineTune, self).__init__()\n",
    "        \n",
    "        # Camada BERT (Será treinada)\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        bert_output_dim = self.bert.config.hidden_size # 768\n",
    "        \n",
    "        # Camadas GNN\n",
    "        self.gcn1 = GCNConv(bert_output_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Pooling (Agrega os nós de tokens em um vetor de documento)\n",
    "        self.pool = global_mean_pool\n",
    "        \n",
    "        # Camada de Classificação Final\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, bert_inputs, pyg_batch):\n",
    "        # Move os inputs do BERT para a GPU\n",
    "        input_ids = bert_inputs['input_ids'].to(device)\n",
    "        attention_mask = bert_inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Move os inputs da GNN para a GPU\n",
    "        edge_index = pyg_batch.edge_index.to(device)\n",
    "        batch_map = pyg_batch.batch.to(device)\n",
    "        \n",
    "        # Passa pelo BERT (Fine-tuning)\n",
    "        # Pegando os embeddings de CADA token\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state \n",
    "\n",
    "        # Preparando Features para a GNN\n",
    "        active_token_embeddings = token_embeddings[attention_mask > 0]\n",
    "                \n",
    "        # Passa pela GNN\n",
    "        h = self.gcn1(active_token_embeddings, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # graph_embedding: (Tamanho do Batch, hidden_dim)\n",
    "        graph_embedding = self.pool(h, batch_map)\n",
    "        \n",
    "        # Classificação\n",
    "        graph_embedding = self.dropout(graph_embedding)\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Instanciando o Modelo \n",
    "model = HybridGNN_FineTune(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes, \n",
    "    dropout_prob=0.5\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Modelo {type(model).__name__} instanciado e movido para {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe1a29",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Iniciamos o ciclo de treinamento do modelo híbrido pelo número de EPOCHS definido. Para cada EPOCH, o código itera sobre todos os lotes de dados, passando os inputs pelo modelo (forward pass). Em seguida, ele calcula o loss, propaga esse erro para trás e ajusta os pesos de todas as camadas (incluindo o BERT e a GNN) usando o otimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd284cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Iniciando Treinamento por {NUM_EPOCHS} épocas ---\")\n",
    "print(f\"Otimizador: Adam, LR: {LEARNING_RATE}, Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train() \n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        bert_inputs = batch['bert_inputs']\n",
    "        pyg_batch = batch['pyg_batch']\n",
    "        \n",
    "        # Pega as labels corretas\n",
    "        labels = pyg_batch.y.to(device)\n",
    "        \n",
    "        # Zera os gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(bert_inputs, pyg_batch)\n",
    "        \n",
    "        # Calcula o loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass \n",
    "        loss.backward()\n",
    "        \n",
    "        # Otimiza (Ajuste dos pesos)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Época {epoch+1}, Lote {i+1}/{len(train_loader)} | Loss Lote: {loss.item():.4f}\")\n",
    "\n",
    "    # Fim da EPOCH\n",
    "    end_time = time.time()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nÉPOCA {epoch+1:02d}/{NUM_EPOCHS} Completa\")\n",
    "    print(f\"  Tempo: {epoch_time:.2f}s\")\n",
    "    print(f\"  Loss Média (Treino): {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Treinamento Concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c62feb",
   "metadata": {},
   "source": [
    "### Avaliação de Desempenho\n",
    "\n",
    "***Nessa Etapa:***\n",
    "\n",
    "Executamos o processo de avaliação do modelo treinado. O código coloca o modelo em modo de avaliação. Para cada lote, ele realiza a previsão, armazena os resultados e implementa uma limpeza manual de memória VRAM (torch.cuda.empty_cache()) após cada lote para garantir estabilidade em GPUs com recursos limitados. Ao final do loop, o tempo total é calculado e a acurácia final do modelo no conjunto de teste é exibida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval() \n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Loop de avaliação com limpeza de cache\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    # Adiciona um timer para ver a velocidade\n",
    "    start_time = time.time() \n",
    "    \n",
    "    for i, batch in enumerate(test_loader):\n",
    "        bert_inputs = batch['bert_inputs']\n",
    "        pyg_batch = batch['pyg_batch']\n",
    "        \n",
    "        labels = pyg_batch.y.to(device)\n",
    "        \n",
    "        # Faz a previsão\n",
    "        logits = model(bert_inputs, pyg_batch)\n",
    "        \n",
    "        # Pega a classe com maior pontuação\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        \n",
    "        # Salva para cálculo final\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Deleta as referências aos tensores da GPU\n",
    "        del bert_inputs, pyg_batch, labels, logits, predicted\n",
    "        \n",
    "        # Força o PyTorch a esvaziar o cache da VRAM\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Calcula a Acurácia Final\n",
    "end_time = time.time()\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "\n",
    "print(f\"\\nTempo total de avaliação: {(end_time - start_time):.2f} segundos\")\n",
    "print(\"\\n--- Resultados Finais (com Fine-Tuning) ---\")\n",
    "print(f\"Total de Amostras de Teste: {len(all_true_labels)}\")\n",
    "print(f\"Acurácia no Teste: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho para salvar \n",
    "dataset = \"imdb\"  \n",
    "MODEL_SAVE_PATH = f\"{dataset}_hybrid_gnn_finetuned.pth\"\n",
    "\n",
    "# Isso salva apenas os parâmetros aprendidos, não a arquitetura\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"\\n--- Modelo Salvo ---\")\n",
    "print(f\"Os parâmetros do modelo foram salvos em: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eacc7e",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo Treinado\n",
    "\n",
    "Este código tem como objetivo restaurar o modelo treinado: primeiro recria a arquitetura do modelo híbrido e, em seguida, carrega os pesos previamente salvos do disco. O passo final e obrigatório é colocar o modelo no modo de avaliação, preparando-o para ser usado para testes de desempenho ou inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Caminho do modelo salvo\n",
    "MODEL_SAVE_PATH = \"r8_hybrid_gnn_finetuned.pth\"\n",
    "num_classes = 8\n",
    "\n",
    "# Recria a arquitetura do modelo\n",
    "print(\"Instanciando a arquitetura do modelo...\")\n",
    "loaded_model = HybridGNN_FineTune(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# Carregar os pesos salvos \n",
    "print(f\"Carregando pesos de {MODEL_SAVE_PATH}...\")\n",
    "loaded_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "\n",
    "# Desliga o dropout e outras camadas específicas de treino.\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"\\n--- Modelo Carregado e Pronto! ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
